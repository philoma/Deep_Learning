# Deep Learning

![image](https://github.com/philoma/Deep_Learning/assets/87674698/dc6253e1-765f-4f1d-b294-715efeaf8760)
![image](https://github.com/philoma/Deep_Learning/assets/87674698/049183ca-5080-4638-b4ca-f5ae631cc21f)

Dropout in Neural Network:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/e36e1b0b-35b9-4327-8d6f-ffc0f98b0005)
Link to the original research paper on <a href='https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf'> Dropout</a>

ReLU variants:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/78c3ba42-980e-416c-b6cc-2067d135ffd5)

Convex & Non-convex Optimization problem(SGD with Momentum):
![image](https://github.com/philoma/Deep_Learning/assets/87674698/04667cde-f1dd-42ae-aecc-6b90f2b6928d)

SGD with Momentum (Optimization Technique):-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/941e7ac7-b49e-48db-a27b-1ba297ed03fb)

Sparse(elongated bowl problem) Vs Non-sparse Data Contour:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/8993e2d6-e76b-402c-9dc3-5593bb0f6921)


Batch Gradient Descent Vs Momentum Vs Adagrad Vs RMSProp Vs Adam [Optimizers]
Batch GD:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/3a9f19ca-9984-42be-a73f-12234b791ed4)

Momentum:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/d50b6f83-31d9-4e2d-bdb5-c2a3206514ee)

Adagrad:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/6fe2c068-e33f-459e-9981-d237762f0f4b)

RMSProp:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/2c2ce57e-3195-4862-a068-6fb0b3f5025b)

Adam:-
shows momentum & learning rate decay behaviour both :) 
![image](https://github.com/philoma/Deep_Learning/assets/87674698/6c878a6a-8936-474d-b03b-2089fb1e4818)


