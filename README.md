# Deep Learning

![image](https://github.com/philoma/Deep_Learning/assets/87674698/dc6253e1-765f-4f1d-b294-715efeaf8760)
![image](https://github.com/philoma/Deep_Learning/assets/87674698/049183ca-5080-4638-b4ca-f5ae631cc21f)

Dropout in Neural Network:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/e36e1b0b-35b9-4327-8d6f-ffc0f98b0005)
Link to the original research paper on <a href='https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf'> Dropout</a>

ReLU variants:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/78c3ba42-980e-416c-b6cc-2067d135ffd5)

Convex & Non-convex Optimization problem(SGD with Momentum):
![image](https://github.com/philoma/Deep_Learning/assets/87674698/04667cde-f1dd-42ae-aecc-6b90f2b6928d)

SGD with Momentum (Optimization Technique):-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/941e7ac7-b49e-48db-a27b-1ba297ed03fb)

Sparse(elongated bowl problem) Vs Non-sparse Data Contour:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/8993e2d6-e76b-402c-9dc3-5593bb0f6921)


Batch Gradient Descent Vs Momentum Vs Adagrad Vs RMSProp Vs Adam [Optimizers]
Batch GD:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/3a9f19ca-9984-42be-a73f-12234b791ed4)

Momentum:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/d50b6f83-31d9-4e2d-bdb5-c2a3206514ee)

Adagrad:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/6fe2c068-e33f-459e-9981-d237762f0f4b)

RMSProp:-
![image](https://github.com/philoma/Deep_Learning/assets/87674698/2c2ce57e-3195-4862-a068-6fb0b3f5025b)

Adam:- <br>
shows momentum & learning rate decay behavior both :) 
![image](https://github.com/philoma/Deep_Learning/assets/87674698/6c878a6a-8936-474d-b03b-2089fb1e4818)


Strided(2,2) Convolution with Paddings:- 

![image](https://upload.wikimedia.org/wikipedia/commons/0/04/Convolution_arithmetic_-_Padding_strides.gif)

<a href='https://deeplizard.com/resource/pavq7noze3'> Max Pooling Visualizer</a>


<br>
<h2><b>VGG-16 | CNN model</b></h2>
ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is an annual computer vision competition. Each year, teams compete on two tasks. The first is to detect objects within an image coming from 200 classes, which is called object localization. The second is to classify images, each labeled with one of 1000 categories, which is called image classification. VGG 16 was proposed by Karen Simonyan and Andrew Zisserman of the Visual Geometry Group Lab of Oxford University in 2014 in the paper “VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION”. This model won 1st  and 2nd place in the above categories in the 2014 ILSVRC challenge.
<br> <br>
<br>

![image](https://media.geeksforgeeks.org/wp-content/uploads/20200219152207/new41.jpg)

![image](https://media.geeksforgeeks.org/wp-content/uploads/20200219152327/conv-layers-vgg16.jpg)


# Best real-time examples of RNN
1. <a href='https://text2data.com/Demo'> Sentiment Analysis</a>

2. Next word/sentence suggestion in Gmail
3. <a href='https://milhidaka.github.io/chainer-image-caption'> Image caption generation </a> (may become helpful for blinds by applying text 2 speech later
4. <a href='https://translate.google.co.in'> Google Translate </a> (Language detection -> Machine translation)
5. < a href='https://www.pragnakalp.com/demos/BERT-NLP-QnA-Demo'> Question And Answer Demo Using BERT NLP - English </a>
